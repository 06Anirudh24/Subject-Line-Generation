{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/lov/Documents/New folder (3)/')\n",
    "import project_tests as tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:/Users/lov/Documents/nlp_trans/new.xlsx'\n",
    "import pandas as pd\n",
    "df=pd.read_excel(filename)\n",
    "df=df.dropna()\n",
    "df=df.reset_index()\n",
    "english_sentences=df['English Translated (write translation with Punctuation)']\n",
    "chn_sentences=df['Mandarin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-3dab704e380e>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chn_sentences[i]=w1\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "for i in range(0,chn_sentences.shape[0]):\n",
    "    words = pseg.cut(chn_sentences[i])\n",
    "    w1=''\n",
    "    for w in words:\n",
    "        w1=w1+' '+w.word\n",
    "    chn_sentences[i]=w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample 1:  hello\n",
      "French sample 1:     喂\n",
      "\n",
      "English sample 2:  hello, hello what is your name?\n",
      "French sample 2:     喂   ,       喂   你   叫   什么   名\n",
      "\n",
      "English sample 3:  ah, where do you live ya?\n",
      "French sample 3:     啊   ,       住   在   哪里   呀\n",
      "\n",
      "English sample 4:  I ah,I, I, I live on the campus currently\n",
      "French sample 4:     (   (   那   我   )   )       啊   ,       我   ,       我           我   就   住   在   学校   现在\n",
      "\n",
      "English sample 5:  campus ya, campus?\n",
      "French sample 5:     学校   呀   ,       在   什么   学校\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(5):\n",
    "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('French sample {}:  {}\\n'.format(sample_i + 1, chn_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36377 English words.\n",
      "3795 unique chn words.\n",
      "10 Most common words in the chn dataset:\n",
      "\",\" \"的\" \"-\" \"我\" \"你\" \"啊\" \"是\" \"嗯\" \"了\" \"%\"\n",
      "\n",
      "34979 english words.\n",
      "4618 unique english words.\n",
      "10 Most common words in the english dataset:\n",
      "\"the\" \"you\" \"I\" \"is\" \"to\" \"a\" \"that\" \"of\" \"it\" \"in\"\n"
     ]
    }
   ],
   "source": [
    "chn_words_counter = collections.Counter([word for sentence in chn_sentences for word in sentence.split()])\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in chn_sentences for word in sentence.split()])))\n",
    "print('{} unique chn words.'.format(len(chn_words_counter)))\n",
    "print('10 Most common words in the chn dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*chn_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} english words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique english words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the english dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 42\n",
      "Max chn sentence length: 37\n",
      "English vocabulary size: 3213\n",
      "chn vocabulary size: 3781\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_chn_sentences, preproc_english_sentences, chn_tokenizer, english_tokenizer =\\\n",
    "    preprocess(chn_sentences, english_sentences)\n",
    "    \n",
    "max_chn_sequence_length = preproc_chn_sentences.shape[1]\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "chn_vocab_size = len(chn_tokenizer.word_index)\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max chn sentence length:\", max_chn_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"chn vocabulary size:\", chn_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(input_shape, output_sequence_length, chn_vocab_size, english_vocab_size):\n",
    "\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(chn_vocab_size, 128, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(GRU(256, return_sequences=True))    \n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(input_shape, output_sequence_length, chn_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], go_backwards=True))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 42, 128)           484096    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 42, 256)           296448    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 42, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 42, 3214)          3294350   \n",
      "=================================================================\n",
      "Total params: 4,338,062\n",
      "Trainable params: 4,338,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 256)               198912    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 42, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 42, 256)           394752    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 42, 1024)          263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 42, 3214)          3294350   \n",
      "=================================================================\n",
      "Total params: 4,151,182\n",
      "Trainable params: 4,151,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 42, 128)           484096    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 42, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 42, 256)           296448    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 42, 512)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 42, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 42, 3214)          1648782   \n",
      "=================================================================\n",
      "Total params: 2,759,054\n",
      "Trainable params: 2,759,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_x = pad(preproc_chn_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2]))\n",
    "\n",
    "# TODO: Train the neural network\n",
    "model_1 = model1(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(chn_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "\n",
    "tmp_x = pad(preproc_chn_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))\n",
    "model_2 = model2(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(chn_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "\n",
    "tmp_x = pad(preproc_chn_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2]))\n",
    "model_3 = model3(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(chn_tokenizer.word_index)+1,\n",
    "    len(english_tokenizer.word_index)+1)\n",
    "\n",
    "model_1.summary()\n",
    "model_2.summary()\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 19s 447ms/step - loss: 3.3352 - accuracy: 0.7135 - val_loss: 1.2794 - val_accuracy: 0.8206\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 15s 407ms/step - loss: 1.2777 - accuracy: 0.8113 - val_loss: 1.2348 - val_accuracy: 0.8261\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 14s 401ms/step - loss: 1.1667 - accuracy: 0.8231 - val_loss: 1.2199 - val_accuracy: 0.8303\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 15s 414ms/step - loss: 1.1157 - accuracy: 0.8265 - val_loss: 1.2082 - val_accuracy: 0.8342\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 15s 410ms/step - loss: 1.0593 - accuracy: 0.8310 - val_loss: 1.2217 - val_accuracy: 0.8363\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 15s 405ms/step - loss: 1.0008 - accuracy: 0.8345 - val_loss: 1.2211 - val_accuracy: 0.8371\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 15s 412ms/step - loss: 0.9137 - accuracy: 0.8429 - val_loss: 1.2435 - val_accuracy: 0.8357\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 15s 413ms/step - loss: 0.8801 - accuracy: 0.8428 - val_loss: 1.2607 - val_accuracy: 0.8366\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 15s 405ms/step - loss: 0.8341 - accuracy: 0.8436 - val_loss: 1.2657 - val_accuracy: 0.8369\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 15s 405ms/step - loss: 0.7431 - accuracy: 0.8526 - val_loss: 1.2748 - val_accuracy: 0.8354\n",
      "hello <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(model_1.predict(tmp_x[:1])[0], english_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  80],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[  80],\n",
       "        [  80],\n",
       "        [  24],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   5],\n",
       "        [ 149],\n",
       "        [  25],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  13],\n",
       "        [1186],\n",
       "        [   3],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[1299],\n",
       "        [   5],\n",
       "        [   3],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[  10],\n",
       "        [  23],\n",
       "        [   0],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 20s 394ms/step - loss: 4.0177 - accuracy: 0.7126 - val_loss: 1.4755 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 13s 349ms/step - loss: 1.4580 - accuracy: 0.8132 - val_loss: 1.3730 - val_accuracy: 0.8214\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 13s 354ms/step - loss: 1.2861 - accuracy: 0.8152 - val_loss: 1.2675 - val_accuracy: 0.8236\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 13s 355ms/step - loss: 1.2412 - accuracy: 0.8130 - val_loss: 1.2754 - val_accuracy: 0.8235\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 13s 353ms/step - loss: 1.2366 - accuracy: 0.8122 - val_loss: 1.2947 - val_accuracy: 0.8240\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 12s 347ms/step - loss: 1.2024 - accuracy: 0.8165 - val_loss: 1.3035 - val_accuracy: 0.8247\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 13s 348ms/step - loss: 1.2417 - accuracy: 0.8112 - val_loss: 1.2862 - val_accuracy: 0.8235\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 13s 374ms/step - loss: 1.1898 - accuracy: 0.8163 - val_loss: 1.2921 - val_accuracy: 0.8214\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 13s 349ms/step - loss: 1.2002 - accuracy: 0.8135 - val_loss: 1.3213 - val_accuracy: 0.8242\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 12s 344ms/step - loss: 1.1769 - accuracy: 0.8182 - val_loss: 1.2975 - val_accuracy: 0.8221\n",
      "hello <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "model_3.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(model_3.predict(tmp_x[:1])[0], english_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 23s 556ms/step - loss: 3.5102 - accuracy: 0.7198 - val_loss: 1.3226 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 20s 568ms/step - loss: 1.3485 - accuracy: 0.8116 - val_loss: 1.2643 - val_accuracy: 0.8248\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 21s 586ms/step - loss: 1.2772 - accuracy: 0.8134 - val_loss: 1.2501 - val_accuracy: 0.8249\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 21s 596ms/step - loss: 1.2819 - accuracy: 0.8087 - val_loss: 1.2612 - val_accuracy: 0.8249\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 20s 546ms/step - loss: 1.2325 - accuracy: 0.8148 - val_loss: 1.2489 - val_accuracy: 0.8250\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 20s 543ms/step - loss: 1.2159 - accuracy: 0.8173 - val_loss: 1.2714 - val_accuracy: 0.8246\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 19s 539ms/step - loss: 1.2189 - accuracy: 0.8168 - val_loss: 1.2476 - val_accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 19s 539ms/step - loss: 1.2078 - accuracy: 0.8157 - val_loss: 1.2737 - val_accuracy: 0.8248\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 20s 571ms/step - loss: 1.2186 - accuracy: 0.8141 - val_loss: 1.2528 - val_accuracy: 0.8248\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 21s 572ms/step - loss: 1.1990 - accuracy: 0.8145 - val_loss: 1.3269 - val_accuracy: 0.8204\n",
      "oh <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "tmp_x = pad(preproc_chn_sentences, preproc_english_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[-2], 1))\n",
    "model_2.fit(tmp_x, preproc_english_sentences, batch_size=100, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(model_2.predict(tmp_x[:1])[0], english_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrgen(noc,an,chn_vocab_size):\n",
    "    ga=np.random.randint(an,size=(noc,1))\n",
    "    ga1=np.zeros((noc,np.shape(tmp_x[0,:])[0]))\n",
    "    for i in range(0,noc):\n",
    "        a=np.random.randint(np.shape(tmp_x[0,:])[0],size=(ga[i][0],1))\n",
    "        for j in range(0,ga[i][0]):\n",
    "            ga1[i,j]=np.random.randint(english_vocab_size)\n",
    "    return ga1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga1=chrgen(50,np.shape(tmp_x[0,:])[0],chn_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(model,ga):\n",
    "    wn=''\n",
    "    for i in range(0,np.shape(model.predict(ga))[0]):\n",
    "        if logits_to_text(model.predict(ga)[i], english_tokenizer)=='<PAD>':\n",
    "            wn=wn\n",
    "        else:  \n",
    "            wn=wn+' '+logits_to_text(model.predict(ga)[i], english_tokenizer)\n",
    "    unique_words = dict.fromkeys(wn.split()) \n",
    "    wn=' '.join(unique_words)\n",
    "    return wn        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
