{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar sentences generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook tries to generate similar sentences to a given input sentence. \n",
    "- It makes use of WordNet and GloVe embeddings to arrive at substitute words for candidate words in a sentence.\n",
    "- Similarity threshold, and number of sentences generated can be controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Glove Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import smart_open\n",
    "from sys import platform\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "def prepend_line(infile, outfile, line):\n",
    "    \"\"\" \n",
    "    Function use to prepend lines using bash utilities in Linux. \n",
    "    (source: http://stackoverflow.com/a/10850588/610569)\n",
    "    \"\"\"\n",
    "    with open(infile, 'r') as old:\n",
    "        with open(outfile, 'w') as new:\n",
    "            new.write(str(line) + \"\\n\")\n",
    "            shutil.copyfileobj(old, new)\n",
    "\n",
    "def prepend_slow(infile, outfile, line):\n",
    "    \"\"\"\n",
    "    Slower way to prepend the line by re-creating the inputfile.\n",
    "    \"\"\"\n",
    "    with open(infile, 'r') as fin:\n",
    "        with open(outfile, 'w') as fout:\n",
    "            fout.write(line + \"\\n\")\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "\n",
    "\n",
    "def get_lines(glove_file_name):\n",
    "    \"\"\"Return the number of vectors and dimensions in a file in GloVe format.\"\"\"\n",
    "    with smart_open.smart_open(glove_file_name, 'r') as f:\n",
    "        num_lines = sum(1 for line in f)\n",
    "    with smart_open.smart_open(glove_file_name, 'r') as f:\n",
    "        num_dims = len(f.readline().split()) - 1\n",
    "    return num_lines, num_dims\n",
    "\n",
    "# Input: GloVe Model File\n",
    "# More models can be downloaded from http://nlp.stanford.edu/projects/glove/\n",
    "glove_file='./glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "num_lines, dims = get_lines(glove_file)\n",
    "\n",
    "# Output: Gensim Model text format.\n",
    "gensim_file='./glove.6B/glove.6B.300d.new.txt'\n",
    "gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "\n",
    "# Prepends the line.\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    prepend_line(glove_file, gensim_file, gensim_first_line)\n",
    "else:\n",
    "    prepend_slow(glove_file, gensim_file, gensim_first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(gensim_file,binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK + WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk is used to perform POS tagging\n",
    "import nltk \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pos_identity(pos_tag):\n",
    "    '''\n",
    "    This method returns\n",
    "    1. 'np' for proper nouns, 'n' for all other nouns\n",
    "    2. 'a' for adjectives    \n",
    "    3. 'v' for verbs    \n",
    "    4. 'r' for adverbs    \n",
    "    5. None for all other tags\n",
    "    '''\n",
    "    if pos_tag in ['NNP', 'NNPS']:\n",
    "        return 'np'\n",
    "    elif pos_tag in ['NN', 'NNS']:\n",
    "        return 'n'\n",
    "    elif pos_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return 'a'\n",
    "    elif pos_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return 'v'\n",
    "    elif pos_tag in ['RB', 'RBR', 'RBS']:\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend.n.01\n",
      "a person you know well and regard with affection and trust\n",
      "['he was my best friend at the university']\n",
      "ally.n.02\n",
      "an associate who provides cooperation or assistance\n",
      "[\"he's a good ally in fight\"]\n",
      "acquaintance.n.03\n",
      "a person with whom you are acquainted\n",
      "['I have trouble remembering the names of all my acquaintances', 'we are friends of the family']\n",
      "supporter.n.01\n",
      "a person who backs a politician or a team etc.\n",
      "['all their supporters came out for the game', 'they are friends of the library']\n",
      "friend.n.05\n",
      "a member of the Religious Society of Friends founded by George Fox (the Friends have never called themselves Quakers)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for result in wordnet.synsets('friend'):\n",
    "    print(result.name())\n",
    "    print(result.definition())\n",
    "    print(result.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(word, pos_tag, similarity_threshold):\n",
    "    \n",
    "    '''\n",
    "    This method returns most similar words to the word passed.\n",
    "    \n",
    "    args:    \n",
    "    word = input word\n",
    "    pos_tag = Simple POS tag of the word\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider\n",
    "    \n",
    "    returns:    \n",
    "    a list of similar words, along with the original word\n",
    "    '''\n",
    "    word = lemmatizer.lemmatize(word, pos_tag)\n",
    "    synonyms = [word] \n",
    "    \n",
    "    try:\n",
    "        vector_check = glove_model.get_vector(word)\n",
    "    except:\n",
    "        return synonyms\n",
    "\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas():\n",
    "            if l.name() in synonyms:\n",
    "                continue\n",
    "            try:\n",
    "                if l.name() in synonyms:\n",
    "                    continue\n",
    "                vector_prospect = glove_model.get_vector(l.name())\n",
    "                A = np.array(vector_check)\n",
    "                B = np.array(vector_prospect)\n",
    "                cosine_diff = np.dot(A,B)/(norm(A)*norm(B))\n",
    "                if cosine_diff > similarity_threshold:\n",
    "                    synonyms.append(l.name())\n",
    "            except:                \n",
    "                pass\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(total_synonym_array, position_array, last_position):\n",
    "    '''    \n",
    "    This method returns the next position of word replacement.\n",
    "    \n",
    "    args:\n",
    "    total_synonym_array = Array containing the total length of synonyms\n",
    "    position_array = Array containing current positions\n",
    "    last_position_array = Integer\n",
    "    \n",
    "    returns:\n",
    "    next position to be updated, -1 if all positions are exhausted\n",
    "    '''\n",
    "    new_pos = last_position\n",
    "    for i in range(len(total_synonym_array)):\n",
    "        new_pos = (new_pos + 1) % len(total_synonym_array)\n",
    "        if position_array[new_pos] == -1 or position_array[new_pos] == total_synonym_array[new_pos]:\n",
    "            continue\n",
    "        else:\n",
    "            return new_pos\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_arrays(sentence_combination):\n",
    "    '''\n",
    "    This is a utility method to get position arrays.\n",
    "    \n",
    "    args:    \n",
    "    sentence_combination = [[word], [word1, word2, ]]\n",
    "    \n",
    "    returns:    \n",
    "    two position arrays\n",
    "    '''\n",
    "    total_synonym_array = []\n",
    "    initial_position_array = []    \n",
    "    for each_word_array in sentence_combination:\n",
    "        length = len(each_word_array)\n",
    "        total_synonym_array.append(length)\n",
    "        if length == 1:\n",
    "            initial_position_array.append(-1)\n",
    "        else:\n",
    "            initial_position_array.append(0)\n",
    "    \n",
    "    return total_synonym_array, initial_position_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_alternate_sentence(sentence, num_versions=1, max_changes=1, similarity_threshold=0.7, ignore_stopwords=True, ignore_proper_nouns=True):\n",
    "    '''\n",
    "    This method returns an alternate version(s) of the sentence passed by replacing words with their closest synonyms.\n",
    "    \n",
    "    args:\n",
    "    sentence (String) = the input sentence\n",
    "    num_versions (int) = the number of alternate versions required\n",
    "    max_changes (int) = the maximum number of changes between versions\n",
    "    similarity_threshold (float) = Value between 0 and 1. Indicates the similarity threshold to consider while replacing words\n",
    "    ignore_stopwords (bool) = If True, stopwords will not be considered for replacement\n",
    "    ignore_proper_nouns (bool) = If True, proper nouns will be ignored for replacement\n",
    "    \n",
    "    returns:\n",
    "    list of alternate sentence(s)\n",
    "    '''\n",
    "    alternate_sentences = []    \n",
    "    sentence_combination = []\n",
    "    words = sentence.split()\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Getting Postion tags and possible combinations with synonyms\n",
    "    for each_word_pos in pos_tags:        \n",
    "        word = each_word_pos[0]\n",
    "        pos_tag = each_word_pos[1]\n",
    "        short_pos = fetch_pos_identity(pos_tag) # if POS is noun, adj, adv, or verb - return pos tag.\n",
    "        \n",
    "        if ignore_proper_nouns and 'np' == short_pos:\n",
    "            sentence_combination.append([word])\n",
    "            continue        \n",
    "        if short_pos is not None:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word, short_pos)\n",
    "        else:\n",
    "            word_lemmatized = lemmatizer.lemmatize(word)\n",
    "        if ignore_stopwords and (word_lemmatized in stop_words or word in stop_words):\n",
    "            sentence_combination.append([word])\n",
    "            continue        \n",
    "        if short_pos is not None:\n",
    "            sentence_combination.append(get_related_words(word, short_pos, similarity_threshold))\n",
    "        else:\n",
    "            sentence_combination.append([word])\n",
    "            continue\n",
    "    \n",
    "    # Getting number of possible combinations and indexes\n",
    "    total_synonym_array, position_array = get_position_arrays(sentence_combination)\n",
    "    total_combos_possible = 0\n",
    "    for some_value in total_synonym_array:\n",
    "        if some_value > 1:\n",
    "            total_combos_possible = total_combos_possible + some_value            \n",
    "    total_combos_possible = total_combos_possible - 1\n",
    "    last_position = -1\n",
    "    \n",
    "    # Generating alternative sentences    \n",
    "    for i in range(num_versions):        \n",
    "        if i >= total_combos_possible:\n",
    "            break        \n",
    "        position = get_next_position(total_synonym_array, position_array, last_position)\n",
    "        if position == -1:\n",
    "            break        \n",
    "        alt_sentence = ''\n",
    "        counter = 0\n",
    "        for j in sentence_combination:\n",
    "            alt_sentence = alt_sentence + ' '\n",
    "            if counter == position:\n",
    "                alt_sentence = alt_sentence + j[position_array[position]]\n",
    "                position_array[position] = position_array[position] + 1                \n",
    "                last_position = position\n",
    "            else:\n",
    "                if position_array[counter] > -1:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter] - 1]\n",
    "                else:\n",
    "                    alt_sentence = alt_sentence + j[position_array[counter]]\n",
    "            counter = counter + 1        \n",
    "        alt_sentence = alt_sentence.strip()\n",
    "        alternate_sentences.append(alt_sentence)\n",
    "    return alternate_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We collect your data regularly',\n",
       " 'We collect your information regularly',\n",
       " 'We accumulate your information regularly',\n",
       " 'We accumulate your info regularly']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provide_alternate_sentence('We collect your information regularly', num_versions=4, similarity_threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mluser/users/data/subject_line\n"
     ]
    }
   ],
   "source": [
    "ROOT = '/home/mluser/users'\n",
    "data_subjects = os.path.join(ROOT, 'data', 'subject_line')\n",
    "print(data_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for (r, d, f) in os.walk(data_subjects):\n",
    "    df = pd.read_parquet(os.path.join(r, f[15]))\n",
    "    print(len(f))\n",
    "#     for i in f:\n",
    "#         result.append(pd.read_parquet(os.path.join(r, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(set(df['subject'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135047"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for d in data:\n",
    "    k = [l for l in d if l==\"{\" or l==\"}\" or l==\"[\" or l==\"]\" or l==\"(\" or l==\")\"]\n",
    "    if len(k)<4:\n",
    "        if \"Lewisville\" not in d:\n",
    "            final.append(d.strip())\n",
    "data = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterSubject(p):\n",
    "    return len(p.split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterSubjects(subs):\n",
    "    return [sub for sub in subs if filterSubject(sub)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93660\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "data = filterSubjects(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [normalizeString(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87843"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(d, n=4, s=0.5):\n",
    "    final = [provide_alternate_sentence(i, num_versions=n, similarity_threshold=s) for i in d]\n",
    "    for i, (d, d1) in enumerate(zip(d, final)):\n",
    "        print('Subject', i,': ', d)\n",
    "        for k in d1:\n",
    "            print(k)\n",
    "        print()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0 :  hurry you re still credit qualified to refinance !\n",
      "hurry you re still credit qualify to refinance !\n",
      "hurry you re however credit qualify to refinance !\n",
      "hurry you re nevertheless credit qualify to refinance !\n",
      "hurry you re yet credit qualify to refinance !\n",
      "\n",
      "Subject 1 :  new york and company off cash back\n",
      "new york and company off cash back\n",
      "\n",
      "Subject 2 :  unitedhealth group health wellness benefits summary plan description\n",
      "unitedhealth group health wellness benefit summary plan description\n",
      "unitedhealth group health wellness benefit summary program description\n",
      "\n",
      "Subject 3 :  recordatorio utiliza zelle para enviar y recibir dinero en un momento\n",
      "\n",
      "Subject 4 :   last chance ! weeks of sunday home delivery just \n",
      "last happen ! week of sunday house delivery just\n",
      "last chance ! week of sunday house delivery just\n",
      "last chance ! week of sunday home delivery just\n",
      "end chance ! week of sunday home delivery just\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = visualize(data[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pavan_bits_DS",
   "language": "python",
   "name": "pavan_bits_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
